# -*- coding: utf-8 -*-
"""tc_lstm

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y9IFPOs3-tpY5agQdBFp7fD_ifi2YjJg
"""

!pip install tensorflow-addons

import csv
import tensorflow as tf
import tensorflow_addons as tfa
import numpy as np
import pandas as pd

from keras.initializers import Constant
from keras.preprocessing.text import one_hot, Tokenizer
from keras.models import Sequential,Model
from keras.layers import *
from keras import regularizers

from numpy import array
from numpy import asarray

from tensorflow.python.client import device_lib
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

from google.colab import drive
drive.mount('/content/drive')

data_path = "/content/drive/My Drive/TopicalChat/topical_chat.csv"
glove_path = "/content/drive/My Drive/TopicalChat/glove.42B.300d.txt"

#Loads the whole embedding into memory (pre-defined word embeddings from GloVe)
def load_glove_vectors(word_dict):
  embeddings_index = {} #Embeddings will be kept here.
  f = open(glove_path)
  for line in f:
    values = line.split()
    word = values[0]
    if word not in word_dict.keys():
      continue
    coefs = asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
  f.close()

  embeddings_found = len(embeddings_index)
  print('Loaded %s word vectors.' % embeddings_found)

  #The module below assigns 0 vector to every word that does not exist in GloVe word vectors
  embedding_dimension = len(list(embeddings_index.values())[0]) #Getting the first word vector and checking its size to determine embedding dimensions.
  for word in word_dict.keys():
    if word not in embeddings_index.keys():
      embeddings_index[word] = np.zeros(embedding_dimension) #Setting non-existing words to zero vector.


  print('Initialized zero vectors for %s words.' % (len(embeddings_index) - embeddings_found) )

  return embeddings_index, embedding_dimension

#Creates and returns a [vocab_size x embedding dimension] size embedding matrix.
def create_embedding_matrix(embeddings,vocab_size,embedding_dimension,word_dict):
  embedding_matrix = np.zeros((vocab_size, embedding_dimension))
  for word, i in word_dict.items():
    embedding_vector = embeddings.get(word)
    if embedding_vector is not None:
      embedding_matrix[i] = embedding_vector
  return embedding_matrix

def convert_labels_onehot(labels):
  label_conversion_dict = {'Angry': [0, 0, 0, 0, 0, 0, 0, 1],
                           'Curious to dive deeper': [0, 0, 0, 0, 0, 0, 1, 0],
                           'Disgusted': [0, 0, 0, 0, 0, 1, 0, 0],
                           'Fearful': [0, 0, 0, 0, 1, 0, 0, 0],
                           'Happy': [0, 0, 0, 1, 0, 0, 0, 0],
                           'Neutral': [0, 0, 1, 0, 0, 0, 0, 0],
                           'Surprised': [0, 1, 0, 0, 0, 0, 0, 0],
                           'Sad': [1, 0, 0, 0, 0, 0, 0, 0]
                          }

  i = 0
  converted_labels = np.zeros(shape=(len(labels), 8))
  for label in labels:
    converted_labels[i] = label_conversion_dict[label.strip()]
    i+=1

  return converted_labels

#Finding the length of the longest sentence in "sent_list" in terms of words.
def find_max_sent_length(sent_list):
  max = 0
  for sent in sent_list:
    if len(sent) > max:
      max = len(sent)

  return max

def get_data(data_path):
  #create two empty arrays
  messages = []
  sentiments = []

  #fill the messages and sentiments arrays
  with open(data_path, encoding='utf8') as f:
    reader = csv.reader(f, delimiter='\n')
    for i, line in enumerate(reader):
        #print(i, line)
        index1 = line[0].index(',')
        index2 = line[0].rindex(',')
        message = line[0][index1+1:index2]
        sentiment = line[0][index2+1:]

        #add message and sentiment to arrrays
        messages.append(message)
        sentiments.append(sentiment)


  #delete first lines of both arrays
  messages = messages[1:]
  sentiments = sentiments[1:]

  #combine messages and sentiments into a dataframe
  data_pd = pd.DataFrame({'Message': messages, 'Sentiment': sentiments})
  #print(data_pd.head(25))

  #separate to training and test
  data1 = data_pd.sample(frac=0.9, random_state=25)
  testing_data = data_pd.drop(data1.index)

  #separate the training data to train and validation
  training_data = data1.sample(frac=0.9, random_state=25)
  validation_data = data1.drop(training_data.index)

  #check if the separation is done as expected
  print(f"No. of training examples: {training_data.shape[0]}")
  print(f"No. of testing examples: {testing_data.shape[0]}")
  print(f"No. of validation examples: {validation_data.shape[0]}")

  #convert string labels into integers
  X_train = training_data['Message']
  y_train = convert_labels_onehot(training_data['Sentiment'])

  X_val = validation_data['Message']
  y_val = convert_labels_onehot(validation_data['Sentiment'])

  X_test = testing_data['Message']
  y_test = convert_labels_onehot(testing_data['Sentiment'])
  print(convert_labels_onehot(testing_data['Sentiment']).shape)
  print("\n\n\ny_test\n", y_test.shape)

  all_corpus = np.concatenate((X_train,X_val),axis = 0)

  t = Tokenizer()
  t.fit_on_texts(all_corpus)

  encoded_all = t.texts_to_sequences(all_corpus)
  max_length = find_max_sent_length(encoded_all)

  encoded_X_train = t.texts_to_sequences(X_train)
  encoded_X_val = t.texts_to_sequences(X_val)
  padded_X_train = tf.keras.utils.pad_sequences(encoded_X_train, maxlen=max_length, padding='post')
  padded_X_val = tf.keras.utils.pad_sequences(encoded_X_val, maxlen=max_length, padding='post')
  encoded_X_test = t.texts_to_sequences(X_test)
  padded_X_test = tf.keras.utils.pad_sequences(encoded_X_test, maxlen=max_length, padding='post')

  #return training, validation and test dataframes
  return padded_X_train,y_train,padded_X_val,y_val,padded_X_test,y_test,t.word_index,max_length

X_train,y_train,X_validation,y_validation,X_test,y_test,word_dict,max_length = get_data(data_path)

print("Train_X: " +str(X_train.shape) + " Val_X: " + str(X_validation.shape) + " Test_X: " + str(X_test.shape))
print("Train_y: " +str(y_train.shape) + " Val_y: " + str(y_validation.shape) + " Test_y: " + str(y_test.shape))

#Prepare embedding matrix where each row represents a word embedding, a [vocab_size x embedding dimension] size matrix.
vocab_size = len(word_dict) + 1
glove_embeddings_dict, embedding_dim = load_glove_vectors(word_dict) #If a word occurs in t.word_index dictionary, then we will have its embedding in glove_embeddings dict.
embedding_matrix = create_embedding_matrix(glove_embeddings_dict, vocab_size, embedding_dim, word_dict)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size,
                              embedding_dim,
                              embeddings_initializer=Constant(embedding_matrix),
                              input_length=max_length,
                              trainable=False),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),
    tf.keras.layers.Dense(embedding_dim, activation='relu'),
    # Add a Dense layer with 8 units and softmax activation.
    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.
    tf.keras.layers.Dense(8, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tfa.metrics.F1Score(num_classes=8, average='micro')])

model.summary()

#Train the model
train_history1 = model.fit(X_train,
                           y_train,
                           validation_data = (X_validation, y_validation),
                           batch_size = 64,
                           epochs = 50,
                           verbose = 1)

#Test the model
test_history1 = model.evaluate(X_test, y_test, verbose = 1)

#plots for visualization

df2 = pd.DataFrame(train_history1.history)
df2.head()

loss_plot2 = df2.plot(y=['loss', 'val_loss'], title = 'Loss vs Epochs (LR: 0.001)', legend = True)
loss_plot2.set(xlabel = 'Epochs')

f1_plot2 = df2.plot(y=['f1_score', 'val_f1_score'], title = 'F1 vs Epochs (LR: 0.001)', legend = True)
f1_plot2.set(xlabel = 'Epochs')