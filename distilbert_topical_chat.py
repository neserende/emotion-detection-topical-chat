# -*- coding: utf-8 -*-
"""topical_chat

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HdnzlXy8U6CjthYwOgUDSmypy7gx-_QT
"""

!pip install -q transformers
!pip install tensorflow-addons
!pip install imbalanced-learn

import pandas as pd
import numpy as np
import tensorflow as tf
import tensorflow_addons as tfa
import csv
#from sklearn.model_selection import train_test_split
#from imblearn.under_sampling import OneSidedSelection
from sklearn.utils.class_weight import compute_class_weight

from transformers import DistilBertTokenizerFast
from transformers import TFDistilBertForSequenceClassification
from sklearn import preprocessing

import matplotlib.pyplot as plt

from tensorflow.python.client import device_lib
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

from google.colab import drive
drive.mount('/content/drive')

data_path = "/content/drive/My Drive/CS550-TopicalChat/topical_chat.csv"

def convert_labels_onehot(labels):
  label_conversion_dict = {'Angry': [0, 0, 0, 0, 0, 0, 0, 1],
                           'Curious to dive deeper': [0, 0, 0, 0, 0, 0, 1, 0],
                           'Disgusted': [0, 0, 0, 0, 0, 1, 0, 0],
                           'Fearful': [0, 0, 0, 0, 1, 0, 0, 0],
                           'Happy': [0, 0, 0, 1, 0, 0, 0, 0],
                           'Neutral': [0, 0, 1, 0, 0, 0, 0, 0],
                           'Surprised': [0, 1, 0, 0, 0, 0, 0, 0],
                           'Sad': [1, 0, 0, 0, 0, 0, 0, 0]
                          }

  i = 0
  converted_labels = np.zeros(shape=(len(labels), 8))
  for label in labels:
    converted_labels[i] = label_conversion_dict[label.strip()]
    i+=1

  return converted_labels

def convert_labels(labels):
  label_conversion_dict = {'Angry': 0,
                           'Curious to dive deeper': 1,
                           'Disgusted': 2,
                           'Fearful':3,
                           'Happy': 4,
                           'Neutral': 5,
                           'Surprised': 6,
                           'Sad': 7
                          }

  i = 0
  converted_labels = np.zeros(shape=(len(labels)), dtype=int)
  for label in labels:
    converted_labels[i] = label_conversion_dict[label.strip()]
    i+=1

  return converted_labels

def get_data(data_path):
  #create two empty arrays
  messages = []
  sentiments = []

  #fill the messages and sentiments arrays
  with open(data_path, encoding='utf8') as f:
    reader = csv.reader(f, delimiter='\n')
    for i, line in enumerate(reader):
        #print(i, line)
        index1 = line[0].index(',')
        index2 = line[0].rindex(',')
        message = line[0][index1+1:index2]
        sentiment = line[0][index2+1:]
        messages.append(message)
        sentiments.append(sentiment)

  #delete first lines of both arrays
  messages = messages[1:]
  sentiments = sentiments[1:]

  #combine messages and sentiments into a dataframe
  data_pd = pd.DataFrame({'Message': messages, 'Sentiment': sentiments})
  #print(data_pd.head(25))

  #separate to training and test
  data1 = data_pd.sample(frac=0.9, random_state=25)
  testing_data = data_pd.drop(data1.index)

  #separate the training data to train and validation
  training_data = data1.sample(frac=0.9, random_state=25)
  validation_data = data1.drop(training_data.index)

  #check if the separation is done as expected
  print(f"No. of training examples: {training_data.shape[0]}")
  print(f"No. of testing examples: {testing_data.shape[0]}")
  print(f"No. of validation examples: {validation_data.shape[0]}")

  #convert string labels into integers
  lb = preprocessing.LabelBinarizer()

  X_train = training_data['Message']
  #y_train = convert_labels_onehot(training_data['Sentiment'])
  y_train = lb.fit_transform(training_data['Sentiment'])

  X_val = validation_data['Message']
  #y_val = convert_labels_onehot(validation_data['Sentiment'])
  y_val = lb.transform(validation_data['Sentiment'])

  X_test = testing_data['Message']
  #y_test = convert_labels_onehot(testing_data['Sentiment'])
  #print(convert_labels_onehot(testing_data['Sentiment']).shape)
  y_test = lb.transform(testing_data['Sentiment'])

  print("\n\n\ny_test\n", y_test.shape)

  #return training, validation and test data
  return X_train, y_train, X_val, y_val, X_test, y_test

X_train, y_train, X_val, y_val, X_test, y_test = get_data(data_path)

np.unique(y_test)
#print("val:" , val_data.head(5))
#print("test:" , test_data.head(5))
#y_test[:5]

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

train_dialog_encodings = tokenizer(X_train.tolist(), truncation = True, padding = True)
#train_dialog_encodings['input_attention'] = train_dialog_encodings['attention_mask']
#train_dialog_encodings.pop('attention_mask')

train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_dialog_encodings), y_train.tolist()))

val_dialog_encodings = tokenizer(X_val.tolist(), truncation = True, padding = True)
#val_dialog_encodings['input_attention'] = val_dialog_encodings['attention_mask']
#val_dialog_encodings.pop('attention_mask')
val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_dialog_encodings), y_val))

test_dialog_encodings = tokenizer(X_test.tolist(), truncation = True, padding = True)
#test_dialog_encodings['input_attention'] = test_dialog_encodings['attention_mask']
#test_dialog_encodings.pop('attention_mask')
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_dialog_encodings), y_test.tolist()))

test_dataset.element_spec[0]['input_ids']

from transformers import TFTrainer, TFTrainingArguments

training_args = TFTrainingArguments(
    output_dir='./results',
    num_train_epochs=6,
    per_device_train_batch_size=64,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=1e-5,
    eval_steps=100
)

with training_args.strategy.scope():
    trainer_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 8 )

trainer = TFTrainer(
    model=trainer_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics =['accuracy', tfa.metrics.F1Score(num_classes=7, average='macro')]
)
#optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
#model.compile(optimizer=optimizer, loss=model.hf_compute_loss, metrics=['accuracy', tfa.metrics.F1Score(num_classes=8, average='macro')])

model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',
                                                              num_labels=8)

loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08)

#compile the model
model.compile(optimizer=optimizer,loss=loss_fn,metrics=['accuracy', tfa.metrics.F1Score(num_classes=8, average='macro')])

model.summary()

with tf.device('/device:GPU:0'):
  train_history = model.fit(train_dataset.shuffle(1000).batch(64),
              epochs=6,
              batch_size=64,
              validation_data=val_dataset.shuffle(1000).batch(64),
              verbose = 1
              )

#test distilBERT fine tuned
test_for_fine_tuned = model.evaluate(test_dataset.shuffle(1000).batch(64),
                                      batch_size=64,
                                          verbose=1)

#plots for visualization -> not fine tuned

df = pd.DataFrame(train_history.history)
df.head()

loss_plot1 = df.plot(y=['loss', 'val_loss'], title = 'Loss vs Epochs', legend = True)
loss_plot1.set(xlabel = 'Epochs')

f1_plot1 = df.plot(y=['f1_score', 'val_f1_score'], title = 'F1 vs Epochs', legend = True)
f1_plot1.set(xlabel = 'Epochs')

#test distilBERT with classification head, not fine tuned

test_for_distilBERT = distilBERT_for_emotion.evaluate(test_dataset, verbose = 1)

FT_EPOCHS = 4
BATCH_SIZE = 64

# Unfreeze distilBERT layers and make available for training
for layer in distilBERT.layers:
    layer.trainable = True

distilBERT_fine_tune = build_model(distilBERT, 220)

# Recompile model after unfreezing
distilBERT_fine_tune.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
                             loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                             metrics=['accuracy', tfa.metrics.F1Score(num_classes=7, average='micro')])

# Train the model
train_history2 = distilBERT_fine_tune.fit(
    train_dataset,
    epochs = FT_EPOCHS,
    batch_size = BATCH_SIZE,
    validation_data = val_dataset,
    verbose=1
)



#plots for visualization -> fine tuned

df2 = pd.DataFrame(train_history2.history)
df2.head()

loss_plot2 = df2.plot(y=['loss', 'val_loss'], title = 'Loss vs Epochs', legend = True)
loss_plot2.set(xlabel = 'Epochs')

f1_plot2 = df2.plot(y=['f1_score', 'val_f1_score'], title = 'F1 vs Epochs', legend = True)
f1_plot2.set(xlabel = 'Epochs')

test_dialog_encodings